{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ad6c96",
   "metadata": {},
   "source": [
    "# Chess Outcome — Model Training\n",
    "\n",
    "**Purpose:** Train deep-learning models to predict the game result from early-game move sequences (first *N* full moves, as configured).\n",
    "\n",
    "**Inputs:** Clean dataset from Notebook 2 (sequence column + target).\n",
    "\n",
    "**Process:** Load & tokenise → build vocab from train only → pad/truncate to the configured length → stratified split → train model(s) with early stopping and LR scheduling → handle class imbalance (balanced batches or class weights) → evaluate and save artefacts.\n",
    "\n",
    "**Metrics:** Accuracy, Macro-F1, and confusion matrix.\n",
    "\n",
    "**Outputs:** Best model (`results/*.keras`) and run report (`results/seq_report.json`). Seed fixed; GPU used if available (mixed precision optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f594bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: E:\\Github Projects\\chess-outcome-prediction\\data\\chess_games_clean.csv\n",
      "Meta: E:\\Github Projects\\chess-outcome-prediction\\data\\chess_games_clean_meta.json\n",
      "Cutoff: 30 moves (60 plies)\n",
      "Results dir: E:\\Github Projects\\chess-outcome-prediction\\results\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 1. Imports & Paths\n",
    "# ==============================================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1.1 Config\n",
    "# ----------------------------------------------\n",
    "SEED = 42\n",
    "CUTOFF_FULL_MOVES = 30\n",
    "CUTOFF_PLIES = CUTOFF_FULL_MOVES * 2\n",
    "MOVE_COL = f\"moves_first{CUTOFF_FULL_MOVES}_san\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1.2 Reproducibility\n",
    "# ----------------------------------------------\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1.3 Paths\n",
    "# ----------------------------------------------\n",
    "NB_DIR = Path.cwd()\n",
    "DATA_DIR = (NB_DIR / \"../data\").resolve()\n",
    "RESULTS_DIR = (NB_DIR / \"../results\").resolve()\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ELO_FEATS = [\"elo_diff\", \"elo_avg\"]\n",
    "\n",
    "CLEAN_CSV = DATA_DIR / \"chess_games_clean.csv\"\n",
    "META_JSON = DATA_DIR / \"chess_games_clean_meta.json\"\n",
    "\n",
    "globals().setdefault(\"BATCH\", 512)\n",
    "globals().setdefault(\"EPOCHS\", 40)\n",
    "\n",
    "\n",
    "print(\"Data:\", CLEAN_CSV)\n",
    "print(\"Meta:\", META_JSON)\n",
    "print(f\"Cutoff: {CUTOFF_FULL_MOVES} moves ({CUTOFF_PLIES} plies)\")\n",
    "print(\"Results dir:\", RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0e6216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (58758, 60) (12594, 60) (12592, 60)\n",
      "Vocab size: 6156\n",
      "Label map: {'black': 0, 'white': 1}\n",
      "Class weights: {0: 1.0008857697679965, 1: 0.9991157966332257}\n",
      "Samples (train/val/test): 58758 12594 12592\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 2. Load, Tokenise, Split\n",
    "# ==============================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.1 Load\n",
    "# ----------------------------------------------\n",
    "df = pd.read_csv(CLEAN_CSV)\n",
    "expected = {MOVE_COL, \"plies_processed\", \"cutoff_reached\", \"target\"}\n",
    "missing = sorted(list(expected - set(df.columns)))\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "df = df.dropna(subset=[MOVE_COL, \"target\"]).reset_index(drop=True)\n",
    "df = df[df[MOVE_COL].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.2 Labels (binary: white vs black only)\n",
    "# ----------------------------------------------\n",
    "# Keep only non-draw games (should already be true after your 02 notebook change,\n",
    "# but this guards against stale CSVs)\n",
    "df = df[df[\"target\"].isin([\"black\", \"white\"])].reset_index(drop=True)\n",
    "\n",
    "CLASS_ORDER = [\"black\", \"white\"]   # id: 0 = black, 1 = white (keep this stable)\n",
    "label_to_id = {c:i for i,c in enumerate(CLASS_ORDER)}\n",
    "id_to_label = {i:c for c,i in label_to_id.items()}\n",
    "\n",
    "y = df[\"target\"].map(label_to_id).astype(\"int32\")\n",
    "assert set(np.unique(y)) == {0,1}, f\"Unexpected labels in y: {set(np.unique(y))}\"\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.3 Tokenise\n",
    "# ----------------------------------------------\n",
    "def to_tokens(s):\n",
    "    return str(s).split()\n",
    "\n",
    "tokens = df[MOVE_COL].apply(to_tokens)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.4 Split\n",
    "# ----------------------------------------------\n",
    "X_tmp, X_test_tok, y_tmp, y_test = train_test_split(\n",
    "    tokens, y, test_size=0.15, random_state=SEED, stratify=y\n",
    ")\n",
    "X_train_tok, X_val_tok, y_train, y_val = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.1765, random_state=SEED, stratify=y_tmp\n",
    ")  # 0.85 * 0.1765 ≈ 0.15 → 70/15/15\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.5 Vocab (train only)\n",
    "# ----------------------------------------------\n",
    "counter = Counter(t for seq in X_train_tok for t in seq[:CUTOFF_PLIES])\n",
    "vocab = [\"<PAD>\", \"<UNK>\"] + [tok for tok, _ in counter.most_common()]\n",
    "stoi = {t:i for i,t in enumerate(vocab)}\n",
    "PAD_ID, UNK_ID = 0, 1\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.6 Numericalise\n",
    "# ----------------------------------------------\n",
    "def to_ids(seq, max_len=CUTOFF_PLIES):\n",
    "    ids = [stoi.get(t, UNK_ID) for t in seq[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_ID] * (max_len - len(ids))\n",
    "    return np.array(ids, dtype=np.int32)\n",
    "\n",
    "X_train_seq = np.stack([to_ids(s) for s in X_train_tok])\n",
    "X_val_seq   = np.stack([to_ids(s) for s in X_val_tok])\n",
    "X_test_seq  = np.stack([to_ids(s) for s in X_test_tok])\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.7 Class Weights\n",
    "# ----------------------------------------------\n",
    "classes = np.array(sorted(label_to_id.values()))\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.8 Persist Artifacts\n",
    "# ----------------------------------------------\n",
    "(RESULTS_DIR / \"vocab.json\").write_text(json.dumps({\"vocab\": vocab}, ensure_ascii=False))\n",
    "(RESULTS_DIR / \"label_mapping.json\").write_text(json.dumps({\"label_to_id\": label_to_id, \"id_to_label\": id_to_label}, indent=2))\n",
    "(RESULTS_DIR / \"class_weights.json\").write_text(json.dumps(class_weight, indent=2))\n",
    "(RESULTS_DIR / \"dl_config.json\").write_text(json.dumps({\"cutoff_full_moves\": CUTOFF_FULL_MOVES, \"cutoff_plies\": CUTOFF_PLIES, \"pad_id\": PAD_ID, \"unk_id\": UNK_ID}, indent=2))\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2.9 Summary\n",
    "# ----------------------------------------------\n",
    "print(\"Shapes:\", X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"Label map:\", label_to_id)\n",
    "print(\"Class weights:\", class_weight)\n",
    "print(\"Samples (train/val/test):\", len(X_train_seq), len(X_val_seq), len(X_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1074d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq shapes: (58758, 60) (12594, 60) (12592, 60)\n",
      "Num shapes: (58758, 2) (12594, 2) (12592, 2)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 2.10 Numeric features (Elo) — standardise on train only\n",
    "# ==============================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ELO_FEATS = [\"elo_diff\", \"elo_avg\"] if \"ELO_FEATS\" not in globals() else ELO_FEATS\n",
    "assert all(f in df.columns for f in ELO_FEATS), f\"Missing columns: {set(ELO_FEATS)-set(df.columns)}\"\n",
    "\n",
    "X_train_num = df.loc[X_train_tok.index, ELO_FEATS].astype(\"float32\").values\n",
    "X_val_num   = df.loc[X_val_tok.index,   ELO_FEATS].astype(\"float32\").values\n",
    "X_test_num  = df.loc[X_test_tok.index,  ELO_FEATS].astype(\"float32\").values\n",
    "\n",
    "num_scaler = StandardScaler().fit(X_train_num)\n",
    "X_train_num = num_scaler.transform(X_train_num)\n",
    "X_val_num   = num_scaler.transform(X_val_num)\n",
    "X_test_num  = num_scaler.transform(X_test_num)\n",
    "\n",
    "print(\"Seq shapes:\", X_train_seq.shape, X_val_seq.shape, X_test_seq.shape)\n",
    "print(\"Num shapes:\", X_train_num.shape, X_val_num.shape, X_test_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac563f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label uniques: {'train': [0, 1], 'val': [0, 1], 'test': [0, 1]}\n",
      "class_weight: {0: 1.0008857697679965, 1: 0.9991157966332257}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# 2.11 Label sanity & class weights (binary)\n",
    "# ----------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "y_train = np.asarray(y_train, dtype=np.int32)\n",
    "y_val   = np.asarray(y_val,   dtype=np.int32)\n",
    "y_test  = np.asarray(y_test,  dtype=np.int32)\n",
    "\n",
    "valid_classes = np.array([0,1], dtype=np.int32)\n",
    "for split_name, yy in [(\"train\", y_train), (\"val\", y_val), (\"test\", y_test)]:\n",
    "    extra = np.setdiff1d(np.unique(yy), valid_classes)\n",
    "    assert extra.size == 0, f\"{split_name} has unexpected labels: {extra}\"\n",
    "\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=valid_classes, y=y_train)\n",
    "class_weight = {int(c): float(w) for c, w in zip(valid_classes, cw)}\n",
    "\n",
    "print(\"Label uniques:\",\n",
    "      {k: sorted(np.unique(v).tolist()) for k, v in\n",
    "       {\"train\": y_train, \"val\": y_val, \"test\": y_test}.items()})\n",
    "print(\"class_weight:\", class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30024a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary two-input training dataset ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# 2.12 Build tf.data datasets (binary, no oversampling)\n",
    "# ----------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH = 512 if \"BATCH\" not in globals() else BATCH\n",
    "\n",
    "ds_train = (\n",
    "    tf.data.Dataset.from_tensor_slices(((X_train_seq, X_train_num), y_train))\n",
    "    .shuffle(200_000, seed=SEED)\n",
    "    .batch(BATCH)\n",
    "    .prefetch(2)\n",
    ")\n",
    "ds_val  = tf.data.Dataset.from_tensor_slices(((X_val_seq,  X_val_num),  y_val )).batch(BATCH).prefetch(2)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(((X_test_seq, X_test_num), y_test)).batch(BATCH)\n",
    "\n",
    "print(\"Binary two-input training dataset ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be892677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ seq_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tok_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ seq_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_9         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ seq_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ num_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bi_gru              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,144</span> │ tok_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ num_in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gmp                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bi_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gmp[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │ concatenate_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ seq_in (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tok_emb (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m787,968\u001b[0m │ seq_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_9         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ seq_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ num_in (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bi_gru              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m198,144\u001b[0m │ tok_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m4\u001b[0m │ num_in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gmp                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bi_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m192\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ gmp[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m41,088\u001b[0m │ concatenate_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,027,525</span> (3.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,027,525\u001b[0m (3.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,027,525</span> (3.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,027,525\u001b[0m (3.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 228ms/step - accuracy: 0.5212 - auc: 0.5356 - loss: 0.6899 - val_accuracy: 0.5968 - val_auc: 0.6428 - val_loss: 0.6602 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 367ms/step - accuracy: 0.6131 - auc: 0.6585 - loss: 0.6529 - val_accuracy: 0.6023 - val_auc: 0.6517 - val_loss: 0.6565 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 277ms/step - accuracy: 0.6326 - auc: 0.6905 - loss: 0.6344 - val_accuracy: 0.6044 - val_auc: 0.6501 - val_loss: 0.6568 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 360ms/step - accuracy: 0.6516 - auc: 0.7144 - loss: 0.6186 - val_accuracy: 0.6000 - val_auc: 0.6450 - val_loss: 0.6670 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 187ms/step - accuracy: 0.6750 - auc: 0.7434 - loss: 0.5950 - val_accuracy: 0.5997 - val_auc: 0.6388 - val_loss: 0.6738 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 360ms/step - accuracy: 0.7032 - auc: 0.7771 - loss: 0.5645 - val_accuracy: 0.5930 - val_auc: 0.6316 - val_loss: 0.7037 - learning_rate: 5.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 373ms/step - accuracy: 0.7246 - auc: 0.8049 - loss: 0.5352 - val_accuracy: 0.5893 - val_auc: 0.6262 - val_loss: 0.7242 - learning_rate: 5.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 371ms/step - accuracy: 0.7438 - auc: 0.8274 - loss: 0.5090 - val_accuracy: 0.5832 - val_auc: 0.6179 - val_loss: 0.7613 - learning_rate: 5.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 371ms/step - accuracy: 0.7652 - auc: 0.8513 - loss: 0.4789 - val_accuracy: 0.5831 - val_auc: 0.6152 - val_loss: 0.7928 - learning_rate: 2.5000e-04\n",
      "\n",
      "--- Validation ---\n",
      "[val] acc=0.6023  f1_macro=0.5997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       0.62      0.52      0.57      6292\n",
      "       white       0.59      0.68      0.63      6302\n",
      "\n",
      "    accuracy                           0.60     12594\n",
      "   macro avg       0.60      0.60      0.60     12594\n",
      "weighted avg       0.60      0.60      0.60     12594\n",
      "\n",
      "\n",
      "--- Test ---\n",
      "[test] acc=0.5928  f1_macro=0.5905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       0.61      0.52      0.56      6291\n",
      "       white       0.58      0.67      0.62      6301\n",
      "\n",
      "    accuracy                           0.59     12592\n",
      "   macro avg       0.59      0.59      0.59     12592\n",
      "weighted avg       0.59      0.59      0.59     12592\n",
      "\n",
      "\n",
      "Saved best model to: E:\\Github Projects\\chess-outcome-prediction\\results\\best_seq_model.keras\n",
      "Saved label_mapping.json, class_weights.json (if used), and seq_report.json to: E:\\Github Projects\\chess-outcome-prediction\\results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3 · Model Training (Binary: white vs black, draws removed)\n",
    "# ============================================================\n",
    "import os, json, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility & paths\n",
    "# -------------------------\n",
    "SEED = globals().get(\"SEED\", 42)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULTS_DIR = globals().get(\"RESULTS_DIR\", \"results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "BEST_MODEL_PATH = os.path.join(RESULTS_DIR, \"best_seq_model.keras\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Expect the following to already exist from earlier notebook cells\n",
    "#   - X_train_seq, X_val_seq, X_test_seq  (int token ids, padded)\n",
    "#   - X_train_num, X_val_num, X_test_num  (float numeric features)\n",
    "#   - y_train, y_val, y_test              (0=black, 1=white)\n",
    "#   - ds_train, ds_val, ds_test           (tf.data datasets)  [optional]\n",
    "#   - CLASS_ORDER = [\"black\",\"white\"]\n",
    "#   - class_weight                        (dict)              [optional]\n",
    "# If a tf.data pipeline is not present, we'll build one quickly.\n",
    "# ----------------------------------------------------------------\n",
    "for name in [\"X_train_seq\",\"X_val_seq\",\"X_test_seq\",\n",
    "             \"X_train_num\",\"X_val_num\",\"X_test_num\",\n",
    "             \"y_train\",\"y_val\",\"y_test\"]:\n",
    "    assert name in globals(), f\"Missing expected variable: {name}\"\n",
    "\n",
    "SEQ_LEN = int(X_train_seq.shape[1])\n",
    "NUM_NUMERIC = int(X_train_num.shape[1]) if len(X_train_num.shape) == 2 else 0\n",
    "\n",
    "# Infer vocab size safely from training IDs (works even if vocab.json not loaded here)\n",
    "VOCAB_SIZE = int(np.max(X_train_seq)) + 1\n",
    "assert VOCAB_SIZE > 1, \"VOCAB_SIZE must be > 1\"\n",
    "\n",
    "# -------------------------\n",
    "# tf.data (if not provided)\n",
    "# -------------------------\n",
    "if \"ds_train\" not in globals():\n",
    "    BATCH = 512\n",
    "    ds_train = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(((X_train_seq, X_train_num), y_train))\n",
    "        .shuffle(200_000, seed=SEED)\n",
    "        .batch(BATCH)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "    ds_val = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(((X_val_seq, X_val_num), y_val))\n",
    "        .batch(BATCH)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "    ds_test = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(((X_test_seq, X_test_num), y_test))\n",
    "        .batch(BATCH)\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Model backbone\n",
    "# -------------------------\n",
    "EMB = 128\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Sequence branch\n",
    "seq_in = layers.Input(shape=(SEQ_LEN,), dtype=\"int32\", name=\"seq_in\")\n",
    "x = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMB, mask_zero=True, name=\"tok_emb\")(seq_in)\n",
    "x = layers.Bidirectional(layers.GRU(128, return_sequences=True), name=\"bi_gru\")(x)\n",
    "x = layers.GlobalMaxPool1D(name=\"gmp\")(x)\n",
    "x = layers.Dropout(DROPOUT)(x)\n",
    "\n",
    "# Numeric branch (optional; if you genuinely have 0 numeric features, we skip it)\n",
    "if NUM_NUMERIC > 0:\n",
    "    num_in = layers.Input(shape=(NUM_NUMERIC,), dtype=\"float32\", name=\"num_in\")\n",
    "    n = layers.LayerNormalization()(num_in)\n",
    "    n = layers.Dense(64, activation=\"relu\")(n)\n",
    "    n = layers.Dropout(DROPOUT)(n)\n",
    "    fused = layers.Concatenate()([x, n])\n",
    "    inputs = [seq_in, num_in]\n",
    "else:\n",
    "    fused = x\n",
    "    inputs = [seq_in]\n",
    "\n",
    "# Head\n",
    "fused = layers.Dense(128, activation=\"relu\")(fused)\n",
    "fused = layers.Dropout(DROPOUT)(fused)\n",
    "out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(fused)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=out)\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Compile\n",
    "# -------------------------\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=7, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-5\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        BEST_MODEL_PATH, monitor=\"val_loss\", save_best_only=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "EPOCHS = globals().get(\"EPOCHS\", 40)\n",
    "\n",
    "# use class_weight if earlier cell computed it; otherwise None\n",
    "cw = globals().get(\"class_weight\", None)\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=cw,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation helpers\n",
    "# -------------------------\n",
    "def eval_ds(m, ds, y_true, name, thr=0.5):\n",
    "    y_prob = m.predict(ds, verbose=0).ravel()\n",
    "    y_pred = (y_prob >= thr).astype(\"int32\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    rep = classification_report(y_true, y_pred, target_names=CLASS_ORDER, zero_division=0)\n",
    "    cm  = confusion_matrix(y_true, y_pred).tolist()\n",
    "    print(f\"[{name}] acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(rep)\n",
    "    return {\"acc\": acc, \"f1_macro\": f1m, \"cm\": cm}\n",
    "\n",
    "print(\"\\n--- Validation ---\")\n",
    "val_stats  = eval_ds(model, ds_val,  y_val,  \"val\",  thr=0.5)\n",
    "\n",
    "print(\"\\n--- Test ---\")\n",
    "test_stats = eval_ds(model, ds_test, y_test, \"test\", thr=0.5)\n",
    "\n",
    "# -------------------------\n",
    "# Save lightweight artefacts\n",
    "# -------------------------\n",
    "# Label mapping (2-class)\n",
    "label_mapping = {\"black\": 0, \"white\": 1}\n",
    "with open(os.path.join(RESULTS_DIR, \"label_mapping.json\"), \"w\") as f:\n",
    "    json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "# Class weights (if any)\n",
    "if cw is not None:\n",
    "    with open(os.path.join(RESULTS_DIR, \"class_weights.json\"), \"w\") as f:\n",
    "        json.dump({int(k): float(v) for k, v in cw.items()}, f, indent=2)\n",
    "\n",
    "# Simple metrics record\n",
    "with open(os.path.join(RESULTS_DIR, \"seq_report.json\"), \"w\") as f:\n",
    "    json.dump({\"val\": val_stats, \"test\": test_stats}, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved best model to:\", BEST_MODEL_PATH)\n",
    "print(\"Saved label_mapping.json, class_weights.json (if used), and seq_report.json to:\", RESULTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
